# AI/LLM Crawler Support Files

This document lists all files created to help AI/LLM crawlers understand and index the site.

## Files Created

### 1. `/llms.txt` ✅
- **Location**: `public/llms.txt`
- **URL**: `https://dylanyoung.dev/llms.txt`
- **Purpose**: Comprehensive information about site structure, content types, SEO/GEO optimizations, and technical details
- **Content**: Site info, structure, content types, technical stack, SEO metadata, author info

### 2. `/humans.txt` ✅
- **Location**: `public/humans.txt`
- **URL**: `https://dylanyoung.dev/humans.txt`
- **Purpose**: Information about the people behind the website
- **Content**: Developer info, credits, site standards, contact info

### 3. `/.well-known/ai.txt` ✅
- **Location**: `public/.well-known/ai.txt`
- **URL**: `https://dylanyoung.dev/.well-known/ai.txt`
- **Purpose**: Specific information for AI crawlers and LLMs
- **Content**: Site overview, author expertise, content structure, crawling guidelines

### 4. `/.well-known/security.txt` ✅
- **Location**: `public/.well-known/security.txt`
- **URL**: `https://dylanyoung.dev/.well-known/security.txt`
- **Purpose**: Security contact information and policy
- **Content**: Contact email, expiration date, security policy

### 5. `/feed.xml` (RSS Feed) ✅
- **Location**: Generated in `public/feed.xml` during build
- **URL**: `https://dylanyoung.dev/feed.xml`
- **Purpose**: RSS feed for content discovery and syndication
- **Content**: Latest 20 blog posts with metadata
- **Generation**: Runs automatically during `postbuild` script

### 6. Enhanced `robots.txt` ✅
- **Location**: Generated by `next-sitemap` in `public/robots.txt`
- **URL**: `https://dylanyoung.dev/robots.txt`
- **Purpose**: Crawling rules for search engines and AI bots
- **Enhancements**: 
  - Explicitly allows AI crawlers (GPTBot, ChatGPT-User, CCBot, anthropic-ai, Claude-Web, Google-Extended)
  - References RSS feed in sitemap
  - Better crawling policies

### 7. Enhanced `sitemap.xml` ✅
- **Location**: Generated by `next-sitemap` in `public/sitemap.xml`
- **URL**: `https://dylanyoung.dev/sitemap.xml`
- **Enhancements**:
  - Includes RSS feed reference
  - Better change frequencies
  - Priority settings

## How It Works

### Build Process
1. Next.js builds the static site
2. `next-sitemap` generates `sitemap.xml` and `robots.txt`
3. `generate-rss.js` script fetches posts from Sanity and generates `feed.xml`
4. All files are placed in `public/` directory for static serving

### AI Crawler Benefits

1. **Better Understanding**: AI crawlers can understand:
   - Site structure and content organization
   - Author expertise and credentials
   - Content types and topics
   - Geographic targeting

2. **Content Discovery**: 
   - RSS feed provides structured content feed
   - Sitemap provides all page URLs
   - robots.txt guides crawling behavior

3. **SEO/GEO Optimization**:
   - Structured data (JSON-LD) helps AI understand content
   - Geographic metadata for regional targeting
   - Proper meta tags for social sharing

## Testing

You can test these files by visiting:
- `https://dylanyoung.dev/llms.txt`
- `https://dylanyoung.dev/humans.txt`
- `https://dylanyoung.dev/.well-known/ai.txt`
- `https://dylanyoung.dev/.well-known/security.txt`
- `https://dylanyoung.dev/feed.xml`
- `https://dylanyoung.dev/robots.txt`
- `https://dylanyoung.dev/sitemap.xml`

## Maintenance

- **RSS Feed**: Automatically regenerated on each build with latest posts
- **Sitemap**: Automatically updated by next-sitemap
- **Other files**: Static files, update manually as needed

## Additional Recommendations

1. **Monitor**: Check Google Search Console for AI crawler activity
2. **Update**: Keep llms.txt and ai.txt updated as site evolves
3. **Verify**: Test RSS feed validates correctly (use RSS validators)
4. **Analytics**: Track which AI crawlers are accessing your site

